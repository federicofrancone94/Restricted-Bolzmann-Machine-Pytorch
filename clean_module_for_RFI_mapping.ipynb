{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark Context and Spark Configuration\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.SparkEnv import sparkSess\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.15.4\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 97kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "Successfully installed numpy-1.14.5\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 20.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo python3 -m pip install numpy==1.15.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import UserDefinedFunction, udf, struct\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#\n",
    "#sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType, DataType\n",
    "from pyspark.sql.functions import col, when, lit, concat_ws\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = \"hdfs:///input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 items\n",
      "-rw-r--r--   1 hadoop hadoop    6260261 2020-08-12 16:07 hdfs:///input/featureIndex.csv\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:06 hdfs:///input/feature_data\n",
      "-rw-r--r--   1 hadoop hadoop 7653991756 2020-08-12 16:09 hdfs:///input/ff_dsp_non_addressable_model_acx_tbl.csv\n",
      "-rw-r--r--   1 hadoop hadoop 2530566106 2020-08-12 16:19 hdfs:///input/ff_dsp_non_addressable_model_aff_tbl.csv\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:08 hdfs:///input/gdsa_conversion_data\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:08 hdfs:///input/gdsa_data\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:10 hdfs:///input/gdsa_data_acxiom_enh\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:11 hdfs:///input/gdsa_data_piq\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:10 hdfs:///input/gdsa_data_tu\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:11 hdfs:///input/gdsa_data_zcodes\n",
      "-rw-r--r--   1 hadoop hadoop 1054158304 2020-08-12 16:07 hdfs:///input/md5_match_data.csv\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-08-12 16:07 hdfs:///input/user_requests_10mil\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls $pre/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+----------+\n",
      "|             _c0|             _c1|   EMAIL_ADDRESS_MD5|Convertion|\n",
      "+----------------+----------------+--------------------+----------+\n",
      "|07DPUS034TKKDPL3|07DPUS02VVVN4FTW|1866864cc75b4c559...|         1|\n",
      "|07DPUS03ZVDM08L2|07DPUS1155V4LHLY|1868a51c468a6706b...|         1|\n",
      "|07DPUS13HRSDKX79|07DPUS02ZCMMXJBE|1868d069aba4d307d...|         1|\n",
      "|07DPUS03VP4VD368|07DPUS1179GDVR7Q|1869eda66f9acaae9...|         1|\n",
      "|07DPUS03MPH4J35P|07DPUS01MZVR3EEH|186b2441c7d2183f2...|         1|\n",
      "+----------------+----------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversions = sparkSess.read.csv('hdfs:///input/gdsa_conversion_data', header= False).withColumn('Convertion', F.lit(1))  \\\n",
    "                                         .withColumnRenamed('_c2', 'EMAIL_ADDRESS_MD5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8632594"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conversions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.when(condition, value)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hadoop fs -put ./notebooks/tu_mapping.csv hdfs:///input/     #to put csv in hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = \"hdfs:///input\"\n",
    "acxiom_path= f\"{pre}/ff_dsp_non_addressable_model_acx_tbl.csv\"\n",
    "tu_path= 'hdfs:///input/gdsa_data_tu/'\n",
    "all_rfi_path = f\"{pre}/user_requests_10mil\"\n",
    "md5_rfi_mappings_path= f\"{pre}/md5_match_data.csv/\"\n",
    "\n",
    "tu_mapping_path= 'hdfs:///input/tu_mapping.csv'\n",
    "conversion_path= 'hdfs:///input/gdsa_conversion_data'\n",
    "\n",
    "## only needed to export\n",
    "output_path= 'hdfs:///output_md5_mapping/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = mapping.get_final_mapping()\n",
    "# temp.persist()\n",
    "# temp.agg(F.countDistinct(\"RFI\"), F.countDistinct(\"MD5\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the class with Convertion mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping_rfi_md5_conv:\n",
    "    def __init__(self, \n",
    "                 acxiom_path, \n",
    "                 tu_path,\n",
    "                 all_rfi_path,\n",
    "                 md5_rfi_mappings_path,\n",
    "                 conversion_path,\n",
    "                 output_path):  \n",
    "        \n",
    "        self.na_values = ['-99', '\\\\N', '\\\\\\\\N', 'Missing', 'missing', 'NA', '?', '.', 'NULL', '', ' ']\n",
    "        \n",
    "        self.acxiom = sparkSess.read.csv(acxiom_path, header=True, nullValue= self.na_values)\n",
    "        self.tu = sparkSess.read.csv(tu_path, header=True, nullValue= self.na_values)\n",
    "        self.joined_records_DF = self.load_rfi_md5_mapping(all_rfi_path, md5_rfi_mappings_path)\n",
    "        self.conversion_path= conversion_path\n",
    "        self.output_path = output_path\n",
    "  \n",
    "    def execute(self):\n",
    "        md5_with_count_missing_DF = self.generate_md5_count_missing()\n",
    "        ranked_DF = self.filter_by_count_missing_rank(md5_with_count_missing_DF, self.conversion_path)\n",
    "        ranked_DF.write.parquet(self.output_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_rfi_md5_mapping(all_rfi_path, md5_rfi_mappings_path):\n",
    "        all_rfi_DF = sparkSess.read.csv(all_rfi_path, sep=\"\\t\") \\\n",
    "                           .toDF(*[\"RFI\", \"Request ID\"]) \\\n",
    "                           .select(\"RFI\")\n",
    "        md5_rfi_mappings_DF= sparkSess.read.csv(md5_rfi_mappings_path) \\\n",
    "                               .toDF(*[\"RFI\", \"MD5\"])\n",
    "        \n",
    "        return all_rfi_DF.join(md5_rfi_mappings_DF, on=\"RFI\", how=\"inner\")\n",
    "    \n",
    "    \n",
    "    def tu_transforming(self):\n",
    "        tu_mapping = self.tu_mapping\n",
    "        tu_transformed= self.tu\n",
    "        \n",
    "        cols = map(lambda x: x.lower(), tu_transformed.columns)\n",
    "        \n",
    "        tu_transformed = tu_transformed.toDF(*cols)\n",
    "\n",
    "        tu_mapping = tu_mapping.filter(col('raw_var').isin(*tu_transformed.columns))\n",
    "\n",
    "        binary_cols_df = tu_mapping.filter(col('values') == 'binary').select('raw_var', 'grouped_var').collect()\n",
    "        binary_cols = [(r['raw_var'], r['grouped_var']) for r in binary_cols_df]\n",
    "\n",
    "        cols_to_combine_df = tu_mapping.filter(col('values') != 'binary').collect()\n",
    "        cols_to_combine = [(r['raw_var'], r['grouped_var'], r['values']) for r in cols_to_combine_df]\n",
    "\n",
    "        tu_transformed = tu_transformed.select('email_address_md5', \n",
    "                                           *[when(col(col_map[0]).isNull(), 0).otherwise(1).name(col_map[1]) for col_map in binary_cols],\n",
    "                                           *[when(col(col_map[0]) == 1, col_map[2]).otherwise(None).name(col_map[0]) for col_map in cols_to_combine])\n",
    "\n",
    "        for new_var in set([col_map[1] for col_map in cols_to_combine]):\n",
    "            raw_vars = set([col_map[0] for col_map in cols_to_combine if col_map[1] == new_var])\n",
    "            tu_transformed = tu_transformed.withColumn(new_var, concat_ws('_', *raw_vars)).withColumn(new_var, when(col(new_var)!='', col(new_var)).otherwise('Missing'))\n",
    "\n",
    "        raw_vars = set([col_map[0] for col_map in cols_to_combine+binary_cols])\n",
    "        tu_transformed = tu_transformed.drop(*(raw_vars)).withColumnRenamed('email_address_md5', \"EMAIL_ADDRESS_MD5\")\n",
    "        try:\n",
    "            ### these columns create problems bcs of their syntax\n",
    "            tu_transformed= tu_transformed.drop('tu_homehasappreciatedinlast12mos.', 'tu_homehasdepreciatedinlast12mos.')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        tu_mapping.unpersist()\n",
    "        return tu_transformed\n",
    "    \n",
    "        \n",
    "    def generate_md5_count_missing(self):\n",
    "        \"\"\"Full outer join between acxiom and TU; only on the basis of these two we\"\"\"\n",
    "        \n",
    "        #### does not work!!! for now I comment it out\n",
    "#         tu_transformed= self.tu_transforming()\n",
    "\n",
    "\n",
    "        joined_df = self.acxiom.join(self.tu, on= ['EMAIL_ADDRESS_MD5', 'PROFILE_ID'], how= 'full_outer') \\\n",
    "                        .withColumnRenamed(\"EMAIL_ADDRESS_MD5\", \"MD5\")\n",
    "        \n",
    "        acxiom_tu_columns = list(set(joined_df.columns) - set(\"MD5\"))\n",
    "        calc_num_missing = lambda x: F.when(F.col(x).isNull(), 1).otherwise(0)\n",
    "        \n",
    "        return \\\n",
    "          joined_df.join(self.joined_records_DF, on=\"MD5\", how= 'inner') \\\n",
    "                   .withColumn(\"COUNT_MISSING\", sum([calc_num_missing(x) for x in acxiom_tu_columns])) \\\n",
    "                   .select([\"RFI\", \"MD5\", 'COUNT_MISSING'])\n",
    "                  \n",
    "    \n",
    "    def filter_by_count_missing_rank(self, df, conversion_path):\n",
    "        \n",
    "        conversions = sparkSess.read.csv(conversion_path, header= False).withColumn('Conversion', F.lit(1))  \\\n",
    "                                         .withColumnRenamed('_c2', 'EMAIL_ADDRESS_MD5').drop('_c0', '_c1')\n",
    "        \n",
    "        df= df.withColumnRenamed(\"MD5\", \"EMAIL_ADDRESS_MD5\")\n",
    "        df= df.join(conversions, on='EMAIL_ADDRESS_MD5', how= 'left') \\\n",
    "                                                .fillna(0, subset=['Conversion'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        df.createOrReplaceTempView(\"to_rank_official\")\n",
    "\n",
    "        ranked_final = sparkSess.sql(\" SELECT RFI, COUNT_MISSING, EMAIL_ADDRESS_MD5, CONVERSION, \\\n",
    "                        rank() over (PARTITION BY RFI ORDER BY count_missing asc, EMAIL_ADDRESS_MD5) AS RANK \\\n",
    "                        FROM to_rank_official  \")\n",
    "        \n",
    "        \n",
    "        ### I get for each RFI the maximum conversion, since if at least 1 household converted, it s ok\n",
    "        propagating_convertion= ranked_final.groupby('RFI')      \\\n",
    "                                            .agg((F.max('Conversion'))   \\\n",
    "                                                .alias('Max_Conversion'))\n",
    "        \n",
    "        \n",
    "        propagating_convertion.createOrReplaceTempView(\"propagating_convertion\")\n",
    "        \n",
    "        ranked_final.createOrReplaceTempView(\"to_rank_official\")\n",
    "        propagating_convertion.createOrReplaceTempView(\"propagating_convertion\")\n",
    "        \n",
    "        ranked_final = sparkSess.sql(\" SELECT A.RFI, COUNT_MISSING, EMAIL_ADDRESS_MD5, MAX_CONVERSION, \\\n",
    "                                rank() over (PARTITION BY A.RFI ORDER BY count_missing asc, EMAIL_ADDRESS_MD5) AS RANK \\\n",
    "                                FROM to_rank_official  A   \\\n",
    "                                INNER JOIN propagating_convertion B \\\n",
    "                                ON A.RFI==B.RFI \")\n",
    "        \n",
    "        \n",
    "        ranked_final.createOrReplaceTempView(\"ranked_table\")\n",
    "\n",
    "\n",
    "        final_mapping_rfi_md5 = sparkSess.sql(\" SELECT RFI, EMAIL_ADDRESS_MD5, MAX_CONVERSION \\\n",
    "                                            FROM ranked_table   \\\n",
    "                                            WHERE RANK == 1 \" )\n",
    "        \n",
    "        return final_mapping_rfi_md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_conv = Mapping_rfi_md5_conv(acxiom_path=acxiom_path,\n",
    "                          tu_path=tu_path, \n",
    "                          all_rfi_path=all_rfi_path, \n",
    "                          md5_rfi_mappings_path=md5_rfi_mappings_path,\n",
    "                          conversion_path= conversion_path ,         \n",
    "                          output_path=\"hdfs:///md5_mapping_with_conv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_conv.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                RFI|                 MD5|\n",
      "+-------------------+--------------------+\n",
      "|1001558335910283967|30df85ec60751d05b...|\n",
      "|1001558335910283967|bf3558b830afd37f4...|\n",
      "|1001558335910283967|12842a3ce3e3ed472...|\n",
      "|1001558335910283967|a96b6d100ba9de09e...|\n",
      "|1001558335916624266|3d9d18dcc821e481d...|\n",
      "|1001558335916624266|e4f068b9c0f4ef987...|\n",
      "|1001558335916624266|abe024ad5858bbee6...|\n",
      "|1001558335916624266|20348bee6be5afa00...|\n",
      "|1001558335916624266|1e9b88cfda2c212ef...|\n",
      "|1001558335916624266|a82a908be803a615f...|\n",
      "|1001558335916624266|6c37e143991612903...|\n",
      "|1001558335916624266|780153f39b9c3cc34...|\n",
      "|1001558335931974134|d77d018812065735d...|\n",
      "|1001558335931974134|5d679ce6c149f30f1...|\n",
      "|1001558335931974134|5dcc11d2f012af408...|\n",
      "|1001558335931974134|268a6eeb1ce696600...|\n",
      "|1001558335931974134|ab3957c9784369158...|\n",
      "|1001558335931974134|a46f9685deb710d91...|\n",
      "|1001558335931974134|3ed348e7215b4cd25...|\n",
      "|1001558335931974134|dc38adef06d964217...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1= mapping_conv.load_rfi_md5_mapping(all_rfi_path=all_rfi_path, \n",
    "                              md5_rfi_mappings_path=md5_rfi_mappings_path,)\n",
    "t1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu_treated = mapping_conv.tu_transforming()\n",
    "# tu_treated.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|                RFI|                 MD5|COUNT_MISSING|\n",
      "+-------------------+--------------------+-------------+\n",
      "|7018250396118575588|00000186b2395bcff...|            0|\n",
      "| 784541142021525012|0000ba03e35b97cb5...|            0|\n",
      "|2339127428260532026|0000df561ad83c980...|            0|\n",
      "|2339971853089377473|00012be322c81e5f1...|            0|\n",
      "|5492586308497501055|0002158ce347ac308...|            0|\n",
      "|5440883427593125031|000296e50bd162d60...|            0|\n",
      "|7290706841403153070|0002c8ccb0d555d05...|          122|\n",
      "| 980447726988504051|00030151cb6f0b79b...|            0|\n",
      "| 970033144884409943|000345c8a6107c58e...|            0|\n",
      "| 770185912496418723|000450d65a1c39faf...|          122|\n",
      "| 958774143348854386|00049a8512d33679e...|            0|\n",
      "| 968344302320862551|00057bd2472c15071...|          122|\n",
      "|2252896107512215054|0005f8ad4e5d70747...|          122|\n",
      "| 978758865116576838|00084f9d2f0546639...|          122|\n",
      "|2339408904093238502|00088a2d83b2fcc92...|            0|\n",
      "| 809592402714003459|000894ac45e6060c9...|            0|\n",
      "|1633322626132376953|0008aec8e65714b6c...|            0|\n",
      "|2810035067613568439|0008aec8e65714b6c...|            0|\n",
      "|1920574141766629642|0009227afe757c60f...|            0|\n",
      "|2340534804618175046|0009252c2fedf96be...|            0|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2= mapping_conv.generate_md5_count_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+\n",
      "|                RFI|   EMAIL_ADDRESS_MD5|MAX_CONVERSION|\n",
      "+-------------------+--------------------+--------------+\n",
      "|1001558335910283967|12842a3ce3e3ed472...|             0|\n",
      "|1001558335916624266|1e9b88cfda2c212ef...|             0|\n",
      "|1001558335931974134|01e93470828bf352f...|             0|\n",
      "|1001558335934013971|3898d46dd10d5684f...|             0|\n",
      "|1001558335950743328|398dd74cd8c29871c...|             0|\n",
      "|1001558336003271199|26f0d2f32b464e72d...|             0|\n",
      "|1001558336009756815|5b1131cddaa367ab0...|             0|\n",
      "|1001558336026378378|495e3aad9ee41ccc5...|             0|\n",
      "|1001558336045992930|1ebfcf9cc2643722e...|             0|\n",
      "|1001558336049231297|812e4ee9277cea75a...|             0|\n",
      "|1001558336114622376|1243923aa40ec51e4...|             0|\n",
      "|1001558336133602821|6b8e362016c03e2d7...|             0|\n",
      "|1001558336171385006|753a2faecddc254d1...|             0|\n",
      "|  10080132718011701|8b6f1e4119e35a680...|             0|\n",
      "|1008690668964788184|a41253e913526ee5c...|             0|\n",
      "|1009065969434392880|2dce0694e02d2d286...|             0|\n",
      "|1009610092229667902|662a537a5d2e73e87...|             0|\n",
      "| 102139484839923342|775219478bc78b193...|             0|\n",
      "|1023048596904495178|58fdb185acdb25a98...|             0|\n",
      "|1027262082842543498|d5ab727cbac6041ae...|             0|\n",
      "+-------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t3= mapping_conv.filter_by_count_missing_rank(df= t2, conversion_path= conversion_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
